models:
  cloud:
    claude-sonnet-3.5:
      provider: anthropic
      model_name: claude-3-5-sonnet-20241022
      max_tokens: 8000
      temperature: 0.7
      specialties:
        - planning
        - complex_reasoning
        - orchestration
      speed: 7
      quality: 10
      cost: 3.0
      context_window: 200000
      
  local:
    mixtral-8x7b:
      provider: ollama
      model_name: mixtral:8x7b
      temperature: 0.6
      specialties:
        - general
        - complex_reasoning
        - hebrew_quality
      speed: 7
      quality: 9
      cost: 0.0
      context_window: 32000
      # RTX 5090 Optimizations
      optimization:
        flash_attention: true
        quantization: "fp16"  # Options: fp16, int8, int4
        batch_size: 4
        max_concurrent: 2
        gpu_memory_fraction: 0.8

    deepseek-r1-32b:
      provider: ollama
      model_name: deepseek-r1:32b
      temperature: 0.3
      specialties:
        - reasoning
        - math
        - logic
        - analysis
      speed: 5
      quality: 9
      cost: 0.0
      context_window: 32000
      # RTX 5090 Optimizations
      optimization:
        flash_attention: true
        quantization: "fp16"
        batch_size: 2
        max_concurrent: 1
        gpu_memory_fraction: 0.9
      
    llama-3.1-8b:
      provider: ollama
      model_name: llama3.1:8b
      temperature: 0.7
      specialties:
        - quick_tasks
        - chat
        - classification
        - simple_queries
      speed: 9
      quality: 7
      cost: 0.0
      context_window: 8000
      
    qwen-2.5-coder-32b:
      provider: ollama
      model_name: qwen2.5-coder:32b
      temperature: 0.2
      specialties:
        - coding
        - debugging
        - code_review
        - code_generation
      speed: 6
      quality: 9
      cost: 0.0
      context_window: 32000
      # RTX 5090 Optimizations
      optimization:
        flash_attention: true
        quantization: "fp16"
        batch_size: 3
        max_concurrent: 2
        gpu_memory_fraction: 0.85

routing:
  default_strategy: quality  # Options: speed, quality, cost
  fallback_model: llama-3.1-8b
  max_retries: 3
  timeout_seconds: 120
  
  task_routing:
    planning:
      - deepseek-r1-32b
    coding:
      - qwen-2.5-coder-32b
    reasoning:
      - deepseek-r1-32b
    quick_response:
      - llama-3.1-8b
    complex_analysis:
      - deepseek-r1-32b
    general:
      - llama-3.1-8b
    understanding task requirements:
      - llama-3.1-8b

